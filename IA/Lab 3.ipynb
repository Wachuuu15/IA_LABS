{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a080ff",
   "metadata": {},
   "source": [
    "Universidad Del Valle de Guatemala\n",
    "\n",
    "Departamento de Ciencias de la Computación\n",
    "\n",
    "Inteligencia Artigicial\n",
    "\n",
    "Laboratorio 3\n",
    "\n",
    "Diana Lucía Fernández Villatoro - 21747\n",
    "\n",
    "Jennifer Michelle Toxcón Ordoñes - 21276\n",
    "\n",
    "Emilio José Solano Orozco - 21212"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53bc70f",
   "metadata": {},
   "source": [
    "Repositorio Git: https://github.com/Wachuuu15/IA_LABS.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98fed40",
   "metadata": {},
   "source": [
    "# TASK 1 - Preguntas Teóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a3724",
   "metadata": {},
   "source": [
    "**1. Explique la diferencia entre descenso de gradiente, descenso de gradiente por mini batches y descenso de gradiente estocástico. Asegúrese de mencionar las ventajas y desventajas de cada enfoque**\n",
    "- _Descenso de gradiente:_ Es un algoritmo de optimización el cual calcula la gradiente en función de pérdida con respecto a parámetros del modelo utilizando todo el conjunto de datos de entrenamiento. Posteriormente se actualizan los parámetros en dirección opuesta al gradiente para que se pueda minimizar la pérdida.\n",
    "\n",
    "    _Ventajas:_\n",
    "    - Se aprovecha la eficiencia de las operaciones vectorizadas en hardware moderno\n",
    "    - Puede converger a la solución global en problemas convexos\n",
    "\n",
    "    _Desventajas:_\n",
    "    - Requiere que el conjunto de datos esté presente en memoria, teniendo una impracticidad con conjuntos de datos grandes.\n",
    "    - La actualización de parámetros ocurre luego de procesar todo el conjunto de datos, teniendo un costo computacional alto.\n",
    "\n",
    "- _Descenso de gradiente por mini batches:_ Esta variante del descenso de gradiente divide el conjunto de datos de entrenamiento en batches mucho más pequeños para calcular la gradiente de cada lote, actualizando los parámetros del modelo luego de cada mini batch.\n",
    "\n",
    "    _Ventajas:_\n",
    "    - Aprovecha la eficiencia de operaciones vectorizadas.\n",
    "    - Maneja conjuntos de datos más grandes.\n",
    "    - Utiliza la paralelización y se beneficia de ella y de la aceleración de hardware moderno.\n",
    "\n",
    "    _Desventajas:_\n",
    "    - Puede no converger tan rápidamente\n",
    "    - Requiere ajuste de hiperparámetros como el tamaño del mini lote \n",
    "\n",
    "- _Descenso de gradiente estocástico:_ Esta versión resulta mucho más eficiente que el algoritmo de descenso de gradiente por mini batches pues, en lugar de utilizar lotes predefinidos, este selecciona un ejemplo de entrenamiento de modo aleatorio en cada iteración, con el cual se le calcula el gradiente y se actualizan los parámetros del modelo.\n",
    "\n",
    "    _Ventajas:_\n",
    "    - Puede converger rápidamente en problemas no convexos y no estacionarios\n",
    "    - Utiliza una menor carga computacional por iteración\n",
    "\n",
    "    _Desventajas:_\n",
    "    - Tiene una mayor viabilidad en las actualizaciones de parámetros debido a la aleatoriedad de la selección de batches\n",
    "    - No aprovecha completamente la eficiencia de las operaciones vectorizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7044b0",
   "metadata": {},
   "source": [
    "**2. Compare y contraste técnicas de extracción de features (feature extraction) y selección de features (feature selection) en machine learning. De ejemplos de escenarios donde cada técnica sería más apropiada**\n",
    "\n",
    "Definiendo la extracción de features como la transformación de los datos originales en un nuevo conjunto de características, generalmente más pequeño y más significativo, y la selección de features como elegir un subconjunto de las características originales para luego eliminar aquellas que se consideran mejor importantes o redundantes. Ambas técnicas buscan mejorar el rendimiento de modelos y la eficiencia computación de ellos, mediante la reducción de la dimensionalidad. \n",
    "\n",
    "Sin embargo, la extracción de features crea nuevas representaciones, mientras que la selección únicamente elige subconjuntos de los datos originales. Por último, vale la pena resaltar que la extracción puede ser más adecuada para capturar información más compleja y abstracta, mientras que la selección se prefiere cuando la interpretación de las características es crucial o se desea mantener la integridad de las características originales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8506dd1",
   "metadata": {},
   "source": [
    "**3. Describa la arquitectura y el funcionamiento de un perceptrón de una sola capa (un tipo de red neuronal sin backpropagation). Explique cómo aprende y la forma en la que actualiza sus parámetros**\n",
    "\n",
    "Este tipo de red neuronal consta únicamente por una capa de entrada y una de salida, el cual es comúnmente utilizado para problemas de clasificación binaria linealmente separables. Su arquitectura está dividida en la capa de entrada y la de salida, donde la capa de entrada consta de nodos que representan características de la entrada del problema y cada nodo está conectado a los demás mediante un peso asociado. Por su parte, la capa de salida consiste en un único nodo que es producido por la salida de la red.\n",
    "\n",
    "Por otro lado, el funcionamiento de esta red está dividido en la inicialización, donde se asignan pesos aleatorios o predefinidos a las conexiones de los nodos, la propagación hacia adelante, la cual calcula la salida el perceptrón mediante la suma del producto de cada entrada con su respectivo peso, el cálculo de error, donde se compara la salida obtenida con la deseada, la actualización de pesos, donde estos se actualizan de acuerdo con la regla de aprendizaje del perceptrón para reducir el error, y la iteración, que es donde los pasos anteriores se repiten para cada muestra de entrenamiento hasta que el error se considera aceptable o hasta que se alcance el número de iteraciones predefinido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa2f879",
   "metadata": {},
   "source": [
    "## TASK 2 - Ejercicios prácticos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c70f77",
   "metadata": {},
   "source": [
    "### Task 2.1 - Gradiente Descendiente Estocástico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a7ac95",
   "metadata": {},
   "source": [
    "Descenso de gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a743ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d0ad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, Y, theta, alpha, numIterations):\n",
    "    Xtrans = X.transpose()\n",
    "    numPoints = len(Y)\n",
    "    \n",
    "    start_time = time.time()  # Guardar tiempo de inicio\n",
    "    for i in range(0, numIterations):\n",
    "        hypothesis = np.dot(X, theta)\n",
    "        loss = hypothesis - Y\n",
    "\n",
    "        cst = np.sum(loss ** 2) / (2 * numPoints)\n",
    "        print(\"Iteration %d | Cost: %f\" % (i, cst))\n",
    "        \n",
    "        gradient = np.dot(Xtrans, loss) / numPoints\n",
    "        theta = theta - alpha * gradient\n",
    "\n",
    "        alpha = alpha / (1 + i)\n",
    "\n",
    "        print(\"Theta en la iteración %d:\" % i, theta)\n",
    "        print(\"Gradiente en la iteración %d:\" % i, gradient)\n",
    "        \n",
    "    end_time = time.time()  # Guardar tiempo de fin\n",
    "    \n",
    "    print(\"Parámetros finales del modelo:\", theta)\n",
    "    execution_time = end_time - start_time  # Calcular tiempo de ejecución\n",
    "    print(\"Tiempo de ejecución:\", execution_time, \"segundos\")\n",
    "    \n",
    "    return theta\n",
    "\n",
    "def genData(numPoints, bias, variance):\n",
    "    X = np.zeros(shape=(numPoints, 2))\n",
    "    Y = np.zeros(shape=numPoints)\n",
    "    for i in range(0, numPoints):\n",
    "        X[i][0] = 1  # Bias term\n",
    "        X[i][1] = i\n",
    "        Y[i] = 2 * i**3 - 3 * i**2 + 5 * i + 3 + np.random.uniform(0, variance)\n",
    "    return X, Y\n",
    "\n",
    "numPoints = 90\n",
    "X, Y = genData(numPoints, 20, 9)\n",
    "\n",
    "a, b = np.shape(X)\n",
    "\n",
    "numIterations = 1000\n",
    "alpha = 0.001 \n",
    "\n",
    "theta = np.ones(2)\n",
    "\n",
    "theta = gradientDescent(X, Y, theta, alpha, numIterations)\n",
    "\n",
    "x_real = np.linspace(0, numPoints, 100)\n",
    "y_real = 2 * x_real**3 - 3 * x_real**2 + 5 * x_real + 3\n",
    "\n",
    "x_approximated = np.linspace(0, numPoints, 100)\n",
    "y_approximated = np.dot(np.column_stack((np.ones(100), x_approximated)), theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786a2e4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x_real, y_real, label='Función Real', color='green')\n",
    "plt.scatter(X[:, 1], Y, color='blue', label='Puntos de Muestra Real')\n",
    "plt.plot(x_approximated, y_approximated, color='red', label='Aproximación por Descenso de Gradiente')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Descenso de Gradiente y Aproximación Polinómica')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aeb905",
   "metadata": {},
   "source": [
    "A través del descenso de gradiente, hemos mejorado la aproximación de nuestro modelo polinómico de tercer grado a los datos reales. Aunque el costo final es elevado, la visualización revela una mejor alineación entre la curva real y la aproximación del modelo. Experimentaremos con más ajustes de hiperparámetros para mejorar aún más la convergencia y el ajuste del modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9cfbc0",
   "metadata": {},
   "source": [
    "Descenso de gradiente estocástico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4bb11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3937abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_function(x):\n",
    "    return 2 * x**3 - 3 * x**2 + 5 * x + 3\n",
    "\n",
    "def derivative_polynomial_function(x):\n",
    "    return np.polyval(np.polyder([2, -3, 5, 3]), x)\n",
    "\n",
    "def cost_function(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2) / 2\n",
    "\n",
    "def stochastic_gradient_descent(initial_x, learning_rate, num_iterations):\n",
    "    x_values = [initial_x]\n",
    "    costs = []\n",
    "\n",
    "    start_time = time.time()  # Guardar tiempo de inicio\n",
    "    for i in range(num_iterations):\n",
    "        current_x = x_values[-1]\n",
    "        gradient = derivative_polynomial_function(current_x)\n",
    "        new_x = current_x - learning_rate * gradient\n",
    "        x_values.append(new_x)\n",
    "\n",
    "        y_pred = polynomial_function(new_x)\n",
    "        cost = cost_function(polynomial_function(current_x), y_pred)\n",
    "        costs.append(cost)\n",
    "        \n",
    "        print(\"Iteración {}: Costo {}\".format(i + 1, cost))\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time  \n",
    "    print(\"Tiempo de ejecución:\", execution_time, \"segundos\")\n",
    "\n",
    "    return np.array(x_values), np.array(costs)\n",
    "\n",
    "initial_guess = 0.0\n",
    "learning_rate = 0.0001\n",
    "num_iterations = 500\n",
    "\n",
    "x_values, costs = stochastic_gradient_descent(initial_guess, learning_rate, num_iterations)\n",
    "y_values_approximated = polynomial_function(x_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505ab3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, num_iterations + 1), costs, label='Costo')\n",
    "plt.xlabel('Iteración')\n",
    "plt.ylabel('Costo')\n",
    "plt.legend()\n",
    "plt.title('Costo en cada Iteración')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5067079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_real, y_real, label='Función Real')\n",
    "plt.scatter(x_values, y_values_approximated, color='green', label='Aproximación por Descenso de Gradiente Estocástico')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Descenso de Gradiente Estocástico y Aproximación Polinómica')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e117893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604d28b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_function(x):\n",
    "    return 2 * x**3 - 3 * x**2 + 5 * x + 3\n",
    "\n",
    "def derivative_polynomial_function(x):\n",
    "    return np.polyval(np.polyder([2, -3, 5, 3]), x)\n",
    "                      \n",
    "def cost_function(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2) / 2\n",
    "\n",
    "\n",
    "def mini_batch_gradient_descent(X, Y, theta, alpha, num_epochs, batch_size):\n",
    "    m = len(Y)\n",
    "    costs = []\n",
    "    start_time = time.time()  \n",
    "    for epoch in range(num_epochs):\n",
    "        indices = np.random.permutation(m)\n",
    "        X_shuffled = X[indices]\n",
    "        Y_shuffled = Y[indices]\n",
    "        \n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            Y_batch = Y_shuffled[i:i+batch_size]\n",
    "            gradient = np.dot(X_batch.T, np.dot(X_batch, theta) - Y_batch) / len(Y_batch)\n",
    "            theta -= alpha * gradient\n",
    "        \n",
    "    \n",
    "        Y_pred = np.dot(X, theta)\n",
    "        cost = cost_function(Y, Y_pred)\n",
    "        costs.append(cost)\n",
    "        \n",
    "        print(\"Epoch {}: Costo {}\".format(epoch + 1, cost))\n",
    "        \n",
    "    end_time = time.time() \n",
    "    execution_time = end_time - start_time  \n",
    "    print(\"Tiempo de ejecución:\", execution_time, \"segundos\")\n",
    "        \n",
    "    return theta, costs\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "X_train = np.random.rand(100, 1) * 10\n",
    "Y_train = polynomial_function(X_train) + np.random.randn(100, 1) * 5\n",
    "\n",
    "\n",
    "X_train_b = np.c_[np.ones((100, 1)), X_train]\n",
    "\n",
    "\n",
    "theta = np.random.randn(2, 1)\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "theta_final, costs = mini_batch_gradient_descent(X_train_b, Y_train, theta, learning_rate, num_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9900ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "X_new_b = np.c_[np.ones((100, 1)), X_new]\n",
    "Y_new = X_new_b.dot(theta_final)\n",
    "\n",
    "plt.plot(X_new, Y_new, \"r-\", label=\"Aproximación\")\n",
    "plt.scatter(X_train, Y_train, alpha=0.5, label=\"Datos de entrenamiento\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title(\"Aproximación por Descenso de Gradiente por Mini-Batches\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddce753",
   "metadata": {},
   "source": [
    "El SGD actualiza los parámetros utilizando un solo ejemplo de datos a la vez, lo que puede llevar a actualizaciones más frecuentes y, por lo tanto, a una convergencia más rápida, especialmente en conjuntos de datos grandes, por lo cual se entiende porque fue el más rápido de los tres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b442ec",
   "metadata": {},
   "source": [
    "Referencias: https://pythonguia.com/scikit-aprender-descenso-de-gradiente/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8509ca38-355b-402f-8b9a-8fcd2df72c32",
   "metadata": {},
   "source": [
    "### Task 2.2 - Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1fa69-e64a-437b-9e30-057c2c9bd2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed9bf9-dc2a-49c1-8af2-a86641a5ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"high_diamond_ranked_10min.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b04459-c186-497c-ad3b-abab9fbed35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df['blueWins'].value_counts()\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd528635-2c7c-4558-a5e3-0fa60ed5bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe = df.drop(['gameId', 'blueWins'], axis=1)\n",
    "\n",
    "feature_labels = df.drop(['gameId', 'blueWins'], axis=1)\n",
    "blueWins = df['blueWins']\n",
    "\n",
    "# Escalamiento de Features\n",
    "feature_labels = pd.DataFrame(StandardScaler().fit_transform(feature_labels), columns=feature_labels.columns)\n",
    "\n",
    "\n",
    "feature_labels.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed662411-0122-4ce5-8b85-81fe680c61ba",
   "metadata": {},
   "source": [
    "Como se vio en el laboratorio anterior, los valores de mayor importancia fueron de los campos 'blueGoldDiff' y 'blueExperienceDiff'.\n",
    "\n",
    "Valores encontrados: Training set shape: (7903, 2); Validation set shape: (988, 2); Test set shape: (988, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5b4f9e-ac23-48b2-9f81-b9a0cb9b7ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "selected_features = ['blueGoldDiff', 'blueExperienceDiff']\n",
    "\n",
    "X_selected = df[selected_features]\n",
    "y = df['blueWins']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_selected_features = model.predict(X_test)\n",
    "accuracy_selected_features = accuracy_score(y_test, y_pred_selected_features)\n",
    "print(f'Exactitud con características seleccionadas: {accuracy_selected_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c9e633-7fe9-432d-9767-ddabf5aa4472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "X = df.drop(['gameId', 'blueWins'], axis=1)\n",
    "y = df['blueWins']\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_filtered = selector.fit_transform(X, y)\n",
    "\n",
    "selected_columns = X.columns[selector.get_support()]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[selected_columns], y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_filtered_features = model.predict(X_test)\n",
    "accuracy_filtered_features = accuracy_score(y_test, y_pred_filtered_features)\n",
    "print(f'Exactitud con características seleccionadas basadas en filtros: {accuracy_filtered_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38b55d4-86d1-4603-a24e-73b679678118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "X = df.drop(['gameId', 'blueWins'], axis=1)\n",
    "y = df['blueWins']\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "selector = RFE(estimator=model, n_features_to_select=2) \n",
    "X_rfe = selector.fit_transform(X, y)\n",
    "\n",
    "selected_columns_rfe = X.columns[selector.get_support()]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[selected_columns_rfe], y, test_size=0.2, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rfe_features = model.predict(X_test)\n",
    "accuracy_rfe_features = accuracy_score(y_test, y_pred_rfe_features)\n",
    "print(f'Exactitud con características seleccionadas basadas en envoltura (RFE): {accuracy_rfe_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8395e-2e9f-4ed3-abee-cdf5c6bf2798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = df.drop(['gameId', 'blueWins'], axis=1)\n",
    "y = df['blueWins']\n",
    "\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear', random_state=42)\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "selected_columns_l1 = X.columns[model.coef_[0] != 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[selected_columns_l1], y, test_size=0.2, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_l1_features = model.predict(X_test)\n",
    "accuracy_l1_features = accuracy_score(y_test, y_pred_l1_features)\n",
    "print(f'Exactitud con características seleccionadas integradas (L1): {accuracy_l1_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a8f017",
   "metadata": {},
   "source": [
    "### Task 2.3 - Perceptrón"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3bc1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce90b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x, y, iteration, learningRate):\n",
    "  samples, features= x.shape\n",
    "  weight= np.zeros(x.shape[1] + 1)\n",
    "  errors_= []\n",
    "\n",
    "  for _ in range(iteration):\n",
    "    error= 0\n",
    "\n",
    "    for xi, target in zip(x, y):\n",
    "      update = learningRate * (target - predict(xi, weight))\n",
    "\n",
    "      weight[1:] += update * xi\n",
    "      weight[0] += update\n",
    "\n",
    "      error += int(update != 0.0)\n",
    "\n",
    "  errors_.append(error)\n",
    "\n",
    "  return weight, errors_\n",
    "\n",
    "def predict(x, weight):\n",
    "  return np.where(netInput(x, weight) >= 0.0, 1, -1)\n",
    "   \n",
    "def netInput(x, weight):\n",
    "  netInput= np.dot(x, weight[1:]) + weight[0]\n",
    "  return netInput\n",
    "\n",
    "#Data set de Iris\n",
    "iris= load_iris()\n",
    "x = iris.data[:, [0, 1]]\n",
    "y= np.where(iris.target == 0, -1, 1)\n",
    "\n",
    "#Datos para le dataset\n",
    "testSize= 0.2\n",
    "randomState= 45\n",
    "\n",
    "#Dividir el dataset en entrenamiento y prueba\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=45)\n",
    "\n",
    "#Entrenar el perceptrón\n",
    "weigths, _= fit(x_train, y_train, 1000, 0.1)\n",
    "\n",
    "#Frontera de decisión\n",
    "plt.scatter(x_train[y_train == 1, 0], x_train[y_train ==1, 1], color= 'violet', marker= '*', label='setosa')\n",
    "plt.scatter(x_train[y_train == -1, 0], x_train[y_train== -1, 1], color= 'green', marker= 'o', label='No setosa')\n",
    "\n",
    "#Minimos y máximos\n",
    "xMin, xMax = x_train[:, 0].min() -1, x_train[:, 0].max() + 1\n",
    "yMin, yMax = x_train[:, 1].min() -1, x_train[:, 1].max() + 1\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(xMin, xMax, 0.02), np.arange(yMin, yMax, 0.02))\n",
    "z= predict(np.c_[xx.ravel(), yy.ravel()], weigths)\n",
    "z= z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, z, alpha=0.3, cmap='coolwarm')\n",
    "\n",
    "#Gráfica\n",
    "plt.xlabel('Septal Length')\n",
    "plt.ylabel('Septal width')\n",
    "plt.legend(loc= 'upper right')\n",
    "plt.title('Gráfica del perceptron con el set de datos Iris')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90af37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Métrica de rendimiento\n",
    "yPred= predict(x_test, weigths)\n",
    "acurracy= np.mean(yPred == y_test)\n",
    "\n",
    "print(acurracy)\n",
    "\n",
    "#Datos para la gráfica\n",
    "labels= [\"Acurracy\", \"Incorrect\"]\n",
    "colors=['#78bb72', '#4c7249']\n",
    "\n",
    "#Gráfica de pastel\n",
    "plt.pie([acurracy, 1- acurracy], labels= labels, autopct= \"%1.1f%%\", colors= colors)\n",
    "plt.title('Métrica de rendimiento para el perceptrón')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc35b5",
   "metadata": {},
   "source": [
    "**¿Por qué eligió esta métrica?**\n",
    "\n",
    "El utilizar una gráfica de pie es una forma sencilla y gráfica de mostrar el rendimiento de un modelo. En este caso en la gráfica se está mostrando el porcentaje de cuántos valores fueron predecidos de manera correcta e incorrecta, lo que nos permite realizar la comparación de estas proporciones y, al ver que el área verde claro, la cual representa los aciertos, se puede decir que el modelo está funcionando de manera correcta, al tener un aproximado de 93.3% de predicciones correctas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd4d9bf-db44-40e1-8210-06f37e72b225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
